{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from torch import cuda\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "import ast\n",
    "from transformers import Trainer, EarlyStoppingCallback\n",
    "from transformers import TrainingArguments\n",
    "from docx import Document\n",
    "from docx.shared import RGBColor\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "from src.models.helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label2id = {'O': 0,\n",
    "#  'I-Treatment': 1,\n",
    "#  'I-Test': 2,\n",
    "#  'I-Problem': 3,\n",
    "#  'I-Background': 4,\n",
    "#  'I-Other': 5}\n",
    "# id2label = {id: tag for tag, id in label2id.items()}\n",
    "\n",
    "# # Print the dictionaries\n",
    "# print(\"label2id:\", label2id)\n",
    "# print(\"id2label:\", id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform(example_batch):\n",
    "#     example_batch['sentence'] = ast.literal_eval(example_batch['sentence'])\n",
    "#     example_batch['tag'] = ast.literal_eval(example_batch['tag'])\n",
    "#     example_batch['tag'] = [label2id[label] for label in example_batch['tag']]\n",
    "#     return example_batch\n",
    "\n",
    "# def align_labels_with_tokens(labels, word_ids):\n",
    "#     new_labels = []\n",
    "#     current_word = None\n",
    "#     for word_id in word_ids:\n",
    "#         if word_id != current_word:\n",
    "#             # Start of a new word!\n",
    "#             current_word = word_id\n",
    "#             label = -100 if word_id is None else labels[word_id]\n",
    "#             new_labels.append(label)\n",
    "#         elif word_id is None:\n",
    "#             # Special token\n",
    "#             new_labels.append(-100)\n",
    "#         else:\n",
    "#             # Same word as previous token\n",
    "#             label = labels[word_id]\n",
    "#             new_labels.append(label)\n",
    "\n",
    "#     return new_labels\n",
    "\n",
    "# def tokenize_and_align_labels(examples, tokenizer):\n",
    "#     tokenized_inputs = tokenizer(\n",
    "#         examples[\"sentence\"], truncation=True, is_split_into_words=True\n",
    "#     )\n",
    "    \n",
    "#     all_labels = examples[\"tag\"]\n",
    "#     new_labels = []\n",
    "#     for i, labels in enumerate(all_labels):\n",
    "#         word_ids = tokenized_inputs.word_ids(i)\n",
    "#         new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "#     tokenized_inputs[\"labels\"] = new_labels\n",
    "#     return tokenized_inputs\n",
    "\n",
    "# def tokenize_and_align_labels_wrapper(examples, tokenizer):\n",
    "#     return tokenize_and_align_labels(examples, tokenizer)\n",
    "\n",
    "# def untokenize_labels_predictions(word_ids, true_labels, predictions):\n",
    "#     untokenized_true_labels = []\n",
    "#     untokenized_predictions = []\n",
    "\n",
    "#     for sublist_word_ids, sublist_true_labels, sublist_predictions in zip(word_ids, true_labels, predictions):\n",
    "#         current_labels = []\n",
    "#         current_predictions = []\n",
    "#         last_word_id = None\n",
    "\n",
    "#         for word_id, label, prediction in zip(sublist_word_ids[1:-1], sublist_true_labels, sublist_predictions):\n",
    "#             # Skip if this word_id is the same as the last one (it's a subword)\n",
    "#             if word_id == last_word_id:\n",
    "#                 continue\n",
    "\n",
    "#             current_labels.append(label)\n",
    "#             current_predictions.append(prediction)\n",
    "#             last_word_id = word_id\n",
    "\n",
    "#         untokenized_true_labels.append(current_labels)\n",
    "#         untokenized_predictions.append(current_predictions)\n",
    "\n",
    "#     return untokenized_true_labels, untokenized_predictions\n",
    "\n",
    "# class TrainingMonitor:\n",
    "#     def __init__(self):\n",
    "#         self.best_f1 = 0\n",
    "#         self.best_confusion_matrix = None\n",
    "\n",
    "#     def compute_metrics_factory(self, word_ids, fold_no, dataset_name):\n",
    "#         # Define the actual compute_metrics function\n",
    "#         def compute_metrics(eval_preds):\n",
    "#             logits, labels = eval_preds\n",
    "#             predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "#             # Remove ignored index (special tokens) and convert to labels\n",
    "#             true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "#             true_predictions = [\n",
    "#                 [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#                 for prediction, label in zip(predictions, labels)\n",
    "#             ]\n",
    "\n",
    "#             untokenized_true_labels, untokenized_predictions = untokenize_labels_predictions(word_ids, true_labels, true_predictions)\n",
    "\n",
    "#             unflat_true = [label for seq in untokenized_true_labels for label in seq]\n",
    "#             unflat_pred = [label for seq in untokenized_predictions for label in seq]\n",
    "#             unreport = classification_report(y_pred=unflat_pred, y_true=unflat_true, output_dict=True)\n",
    "#             unreport['macro_wo_O'] = {'precision': (unreport['I-Background']['precision'] + unreport['I-Other']['precision'] + unreport['I-Problem']['precision'] + unreport['I-Test']['precision'] + unreport['I-Treatment']['precision']) / 5,\n",
    "#             'recall': (unreport['I-Background']['recall'] + unreport['I-Other']['recall'] + unreport['I-Problem']['recall'] + unreport['I-Test']['recall'] + unreport['I-Treatment']['recall']) / 5,\n",
    "#             'f1-score': (unreport['I-Background']['f1-score'] + unreport['I-Other']['f1-score'] + unreport['I-Problem']['f1-score'] + unreport['I-Test']['f1-score'] + unreport['I-Treatment']['f1-score']) / 5,\n",
    "#             'support': (unreport['I-Background']['support'] + unreport['I-Other']['support'] + unreport['I-Problem']['support'] + unreport['I-Test']['support'] + unreport['I-Treatment']['support'])}\n",
    "            \n",
    "#             un_report_df = pd.DataFrame(unreport).round(3).T\n",
    "\n",
    "#             new_f1_score = unreport['macro_wo_O']['f1-score']\n",
    "#             if self.best_f1 < new_f1_score:\n",
    "#                 self.best_f1 = new_f1_score\n",
    "#                 cm = confusion_matrix(y_pred=unflat_pred, y_true=unflat_true)\n",
    "#                 disp = ConfusionMatrixDisplay(cm, display_labels=np.array(['I-Background','I-Other', 'I-Problem', 'I-Test', 'I-Treatment', 'O']))\n",
    "#                 fig, ax = plt.subplots(figsize=(8, 8))\n",
    "#                 disp.plot(ax=ax)\n",
    "                \n",
    "\n",
    "               \n",
    "#                 binary_predictions = ['0' if label == 'O' else '1' for label in unflat_pred]\n",
    "#                 binary_labels = ['0' if label == 'O' else '1' for label in unflat_true]\n",
    "\n",
    "#                 # Generate a classification report\n",
    "#                 binary_classification_report = classification_report(y_true=binary_labels, y_pred=binary_predictions, target_names=['O', 'I'], digits=3, output_dict=True)\n",
    "                \n",
    "#                 # Save the figure to an image file\n",
    "#                 plt.savefig(f'analysis/{dataset_name}/graphs/fold{fold_no}/confusion_matrix.png')\n",
    "#                 plt.close()\n",
    "                \n",
    "#                 with open(f\"analysis/{dataset_name}/reports/fold{fold_no}/multiclass_classification_report.json\", \"w\") as f:\n",
    "#                     json.dump(un_report_df.to_dict(), f, indent=4)\n",
    "#                 with open(f\"analysis/{dataset_name}/reports/fold{fold_no}/binary_classification_report.json\", \"w\") as f:\n",
    "#                     json.dump(binary_classification_report, f, indent=4)\n",
    "                \n",
    "\n",
    "#             return {\n",
    "#                 \"precision\": unreport['macro_wo_O']['precision'],\n",
    "#                 \"recall\": unreport['macro_wo_O']['recall'],\n",
    "#                 \"f1\": unreport['macro_wo_O']['f1-score'],\n",
    "#                 \"accuracy\": unreport['accuracy'],\n",
    "#             }\n",
    "#         return compute_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_dataset_name = 'mtsamples2'\n",
    "\n",
    "model_config = dict(\n",
    "    train_dataset_name='cross-validation',\n",
    "    eval_dataset_name = 'cross-validation',\n",
    "    checkpoint_path = \"models/bert-finetuned-ner/fold0/checkpoint-2826\", # f'models/models_bert-finetuned-ner_fold0_checkpoint-4710-2024-04-11_11-37-04',\n",
    "    epochs = 50,\n",
    "    batch_size = 4,\n",
    "    learning_rate = 1e-5,\n",
    "    fold_range = range(0,5),\n",
    "    output_model_dir = 'models/nooutput'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model_config):\n",
    "    for fold_no in model_config['fold_range']:  \n",
    "        # Load the tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_config['checkpoint_path'])\n",
    "        data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "        # Load the model\n",
    "        model = AutoModelForTokenClassification.from_pretrained(model_config['checkpoint_path'])\n",
    "\n",
    "        # Load the evaluation data\n",
    "\n",
    "        # eval_dataset = load_dataset('csv', data_files=f'data/processed/{eval_dataset_name}/final_eval.csv')\n",
    "        # eval_dataset = eval_dataset.map(transform)\n",
    "        train_dataset = load_dataset('csv', data_files=f'data/processed/{model_config['train_dataset_name']}/train_fold_{fold_no}.csv')\n",
    "        eval_dataset = load_dataset('csv', data_files=f'data/processed/{model_config['eval_dataset_name']}/test_fold_{fold_no}.csv')\n",
    "    \n",
    "    \n",
    "        # combined = concatenate_datasets([d1['train'], d2['train']])\n",
    "        # eval_dataset = DatasetDict({'train': combined})\n",
    "        train_dataset = train_dataset.map(transform)\n",
    "        eval_dataset = eval_dataset.map(transform)\n",
    "        \n",
    "        \n",
    "        print(eval_dataset)\n",
    "        \n",
    "        # word ids of the tokenized mtsamples to find the original words\n",
    "        eval_sentences = eval_dataset['train']['sentence']\n",
    "        eval_word_ids = []\n",
    "        for sentence in eval_sentences:\n",
    "            eval_word_ids.append(tokenizer(sentence, truncation=True, is_split_into_words=True).word_ids())\n",
    "        \n",
    "        train_sentences = train_dataset['train']['sentence']\n",
    "        train_word_ids = []\n",
    "        for sentence in train_sentences:\n",
    "            train_word_ids.append(tokenizer(sentence, truncation=True, is_split_into_words=True).word_ids())\n",
    "\n",
    "  \n",
    "\n",
    "        # Prepare the evaluation dataset\n",
    "        eval_dataset = eval_dataset.map(lambda examples: tokenize_and_align_labels_wrapper(examples, tokenizer), batched=True, remove_columns=eval_dataset['train'].column_names)\n",
    "        train_dataset = train_dataset.map(lambda examples: tokenize_and_align_labels_wrapper(examples, tokenizer), batched=True, remove_columns=train_dataset['train'].column_names)\n",
    "\n",
    "        # split_dataset = eval_dataset['train'].train_test_split(test_size=0.5)\n",
    "\n",
    "\n",
    "        # Define the training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=model_config['output_model_dir'], # path of aft-ft\n",
    "            overwrite_output_dir=True,\n",
    "            per_device_train_batch_size= model_config['batch_size'],\n",
    "            per_device_eval_batch_size= model_config['batch_size'],\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            learning_rate=model_config['learning_rate'],\n",
    "            num_train_epochs=model_config['epochs'],\n",
    "            logging_strategy=\"epoch\",\n",
    "        )\n",
    "\n",
    "        # Initialize the Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset['train'],\n",
    "            eval_dataset=eval_dataset['train'],\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=TrainingMonitor().compute_metrics_factory(eval_word_ids, fold_no, dataset_name=model_config['eval_dataset_name']),\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.01)]\n",
    "        )\n",
    "\n",
    "        # trainer.train()\n",
    "        # Evaluate the model\n",
    "        \n",
    "        trainer.evaluate(eval_dataset['train'])\n",
    "        # print(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'tag'],\n",
      "        num_rows: 99\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'tag'],\n",
      "        num_rows: 99\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'tag'],\n",
      "        num_rows: 99\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'tag'],\n",
      "        num_rows: 99\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'tag'],\n",
      "        num_rows: 99\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison: 5x2cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate 5 different train-test data. Shuffle and split 5 times.\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# # Let's assume you have a pandas DataFrame called 'df' with your dataset.\n",
    "# data1 = pd.read_csv('data/processed/mtsamples2/mtsamples_extra.csv')\n",
    "# data2 = pd.read_csv(\"data/processed/mtsamples2/final_eval.csv\")\n",
    "# df = pd.concat([data1, data2])\n",
    "\n",
    "\n",
    "# # Define the ShuffleSplit object to split the data into 2 parts, for 5 splits\n",
    "# ss = ShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n",
    "\n",
    "# # This will shuffle and split the data 5 times\n",
    "# for fold, (train_index, test_index) in enumerate(ss.split(df)):\n",
    "#     # Split the data\n",
    "#     train_set = df.iloc[train_index]\n",
    "#     test_set = df.iloc[test_index]\n",
    "    \n",
    "#     # Save each fold to CSV\n",
    "#     train_set.to_csv(f'data/processed/cross-validation/train_fold_{fold}.csv', index=False)\n",
    "#     test_set.to_csv(f'data/processed/cross-validation/test_fold_{fold}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.737, 0.687, 0.678, 0.64, 0.666, 0.618, 0.669, 0.6890000000000001, 0.709, 0.6880000000000001]\n",
      "[0.8992363744386391, 0.8899396426626842, 0.8975292578505147, 0.888146956137661, 0.8817851765659587, 0.8742837900198306, 0.8822022022650344, 0.8764889889657232, 0.8834996322147446, 0.8904372192730877]\n"
     ]
    }
   ],
   "source": [
    "# save scores\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# This is the base directory where your fold reports are located.\n",
    "dataset_name = 'cross-validation'\n",
    "base_dir = f'analysis/{dataset_name}/reports'  \n",
    "# Initialize dictionaries to hold all the metric values from each fold.\n",
    "binary_metrics = {\n",
    "    'O': {'precision': [], 'recall': [], 'f1-score': []},\n",
    "    'I': {'precision': [], 'recall': [], 'f1-score': []},\n",
    "    'macro avg': {'precision': [], 'recall': [], 'f1-score': []},\n",
    "    'weighted avg': {'precision': [], 'recall': [], 'f1-score': []}\n",
    "}\n",
    "\n",
    "multiclass_metrics = {\n",
    "    \"macro avg\": {'precision': [], 'recall': [], 'f1-score': []},\n",
    "    \"weighted avg\": {'precision': [], 'recall': [], 'f1-score': []},\n",
    "    \"macro_wo_O\": {'precision': [], 'recall': [], 'f1-score': []}\n",
    "\n",
    "}\n",
    "\n",
    "# Process each fold\n",
    "for fold in range(10):\n",
    "    fold_dir = os.path.join(base_dir, f'fold{fold}')\n",
    "\n",
    "    # Load binary classification report\n",
    "    with open(os.path.join(fold_dir, 'binary_classification_report.json'), 'r') as f:\n",
    "        binary_report = json.load(f)\n",
    "        for category in binary_metrics.keys():\n",
    "            for metric in binary_metrics[category].keys():\n",
    "                binary_metrics[category][metric].append(binary_report[category][metric])\n",
    "\n",
    "    # Load multiclass classification report\n",
    "    multi_df = pd.read_json(os.path.join(fold_dir, 'multiclass_classification_report.json'))\n",
    "  \n",
    "    # with open(os.path.join(fold_dir, 'multiclass_classification_report.json'), 'r') as f:\n",
    "    multiclass_report = multi_df.T.to_dict()\n",
    "    # print(multiclass_report)\n",
    "\n",
    "    for category in multiclass_metrics.keys():\n",
    "\n",
    "        for metric in multiclass_metrics[category].keys():\n",
    "            multiclass_metrics[category][metric].append(multiclass_report[category][metric])\n",
    "\n",
    "print(multiclass_metrics['macro_wo_O']['f1-score'])\n",
    "print(binary_metrics['macro avg']['f1-score'])\n",
    "\n",
    "# rearrange array\n",
    "def rearrange_array(array):\n",
    "    return [x for pair in zip(array[:5], array[5:]) for x in pair]\n",
    "\n",
    "import pickle\n",
    "with open('analysis/statistical_test/bc_multiclass_scores.pkl', 'wb') as f:\n",
    "   pickle.dump(rearrange_array(multiclass_metrics['macro_wo_O']['f1-score']), f)\n",
    "\n",
    "with open('analysis/statistical_test/bc_binary_scores.pkl', 'wb') as f:\n",
    "   pickle.dump(rearrange_array(binary_metrics['macro avg']['f1-score']), f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def combined_ftest_5x2cv(scores1, scores2):\n",
    "    \"\"\"Perform the Combined 5x2CV F-test on two sets of\n",
    "    model scores to compare their performance.\n",
    "\n",
    "    Assume R1CV1, R1CV2, R2CV1 ... order\n",
    "    where R1CV1 is cross-validation repeat 1 and split 1\n",
    "    Args:\n",
    "        scores1 (list): Regressor 1 scores on the splits\n",
    "        scores2 (list): Regressor 2 scores on the splits\n",
    "    Returns:\n",
    "        f_stat (float): The F-statistic\n",
    "        pvalue (float):  If the chosen significance level is larger than\n",
    "        the p-value, we reject the null hypothesis and accept that\n",
    "        there are significant differences in the two compared models.\n",
    "    \"\"\"\n",
    "    variances = []\n",
    "    differences = []\n",
    "    for i in range(5):\n",
    "        scores_diff1 = scores1[i * 2] - scores2[i * 2]\n",
    "        scores_diff2 = scores1[i * 2 + 1] - scores2[i * 2 + 1]\n",
    "        score_mean = (scores_diff1 + scores_diff2) / 2.0\n",
    "        score_var = (scores_diff1 - score_mean) ** 2 + \\\n",
    "            (scores_diff2 - score_mean) ** 2\n",
    "\n",
    "        differences.extend([scores_diff1**2, scores_diff2**2])\n",
    "        variances.append(score_var)\n",
    "\n",
    "    numerator = sum(differences)\n",
    "    denominator = 2 * (sum(variances))\n",
    "    f_stat = numerator / denominator\n",
    "\n",
    "    pvalue = stats.f.sf(f_stat, 10, 5)\n",
    "    return float(f_stat), float(pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 mtsamples-ft: 0.6781 0.031643166718898406\n",
      "Model 2 AFT: 0.8863549240393878 0.007828496371853642\n"
     ]
    }
   ],
   "source": [
    "scores1 = pd.read_pickle('analysis/statistical_test/bc_multiclass_scores.pkl')\n",
    "scores2 = pd.read_pickle('analysis/statistical_test/bc_binary_scores.pkl')\n",
    "\n",
    "# f_stat, p_val = combined_ftest_5x2cv(scores1, scores2)\n",
    "# print(f_stat)\n",
    "# print(p_val)\n",
    "\n",
    "print(\"Model 1 mtsamples-ft:\", np.mean(scores1), np.std(scores1))\n",
    "print(\"Model 2 AFT:\", np.mean(scores2), np.std(scores2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lara-medh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
