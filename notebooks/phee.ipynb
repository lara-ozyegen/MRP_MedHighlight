{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial count of samples: 3006\n",
      "Number of ambiguous samples (duplicated 'Org_Sentence' but not 'Org_Tag'): 212\n",
      "Total samples removed: 213\n",
      "Initial count of samples: 1003\n",
      "Number of ambiguous samples (duplicated 'Org_Sentence' but not 'Org_Tag'): 85\n",
      "Total samples removed: 85\n",
      "Initial count of samples: 968\n",
      "Number of ambiguous samples (duplicated 'Org_Sentence' but not 'Org_Tag'): 0\n",
      "Total samples removed: 0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Get the raw phee dataset. Give it to clinical BERT model. Save the results in csv files.\n",
    "Sentence_ID\n",
    "Org_Sentence: original sentence (the version in phee)\n",
    "Org_Tag: original tag (the version in phee)\n",
    "Sentence: tokenized sentence (with bert tokenizer)\n",
    "Tag: tags that come from clinical BERT model\n",
    "'''\n",
    "files = ['train', 'dev', 'test']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"samrawal/bert-base-uncased_clinical-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"samrawal/bert-base-uncased_clinical-ner\")\n",
    "model.to(device)\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, add_special_tokens=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)[0]\n",
    "    labels = [model.config.id2label[label_id.item()] for label_id in predictions[1:-1].cpu()]  # Exclude [CLS], [SEP]\n",
    "    united_labels = []\n",
    "    united_tokens = []\n",
    "    # ignore words that are split into subwords\n",
    "    \n",
    "    for token, label in zip(tokenizer.tokenize(sentence), labels):\n",
    "        if token.startswith('##'):\n",
    "            united_tokens[-1] = united_tokens[-1] + token[2:]\n",
    "            continue\n",
    "        united_tokens.append(token)\n",
    "        united_labels.append(label)\n",
    "\n",
    "    return united_tokens, united_labels\n",
    "\n",
    "def drop_duplicates(df):\n",
    "    # Initial number of samples\n",
    "    initial_count = len(df)\n",
    "\n",
    "    # Identify duplicates for 'Sentence' and 'Tag' columns\n",
    "    duplicates_both = df.duplicated(subset=['Org_Sentence', 'Org_Tag'], keep=False)\n",
    "\n",
    "    # Identify duplicates only based on 'Sentence'\n",
    "    duplicates_sentence = df.duplicated(subset=['Org_Sentence'], keep=False)\n",
    "\n",
    "    # Filter out the rows where 'Sentence' is duplicated but 'Tag' is not\n",
    "    to_drop =df[duplicates_sentence & ~duplicates_both]\n",
    "\n",
    "    # Count the number of rows to be dropped\n",
    "    drop_count = len(to_drop)\n",
    "\n",
    "    # Drop the identified rows\n",
    "    df = df.drop(to_drop.index)\n",
    "\n",
    "    # Now, drop duplicate rows based on both 'Sentence' and 'Tag'\n",
    "    df = df.drop_duplicates(subset=['Org_Sentence', 'Org_Tag'])\n",
    "\n",
    "    # Calculate the number of samples removed\n",
    "    samples_removed = initial_count - len(df)\n",
    "\n",
    "    # Report the numbers\n",
    "    print(f\"Initial count of samples: {initial_count}\")\n",
    "    print(f\"Number of ambiguous samples (duplicated 'Org_Sentence' but not 'Org_Tag'): {drop_count}\")\n",
    "    print(f\"Total samples removed: {samples_removed}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess(file_name):\n",
    "    # Reading the file and creating a DataFrame\n",
    "    with open(f'data/raw/phee/ace/{file_name}.txt', 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    words, tags, sentence_ids = [], [], []\n",
    "    sentence_id = 0\n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip() == '':  # Check for empty line indicating end of sentence\n",
    "            sentence_id += 1\n",
    "        else:\n",
    "            word, tag = line.strip().split()\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "            sentence_ids.append(sentence_id)\n",
    "\n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame({'Sentence_ID': sentence_ids, 'Org_Sentence': words, 'Org_Tag': tags})\n",
    "\n",
    "    df_s = df.groupby('Sentence_ID').agg({\n",
    "        'Org_Sentence': lambda x: ' '.join(x),\n",
    "        'Org_Tag': lambda x: ' '.join(x)\n",
    "        }).reset_index()\n",
    "\n",
    "    # df_s = df_s.rename(columns={\"Word\": \"Org_Sentence\"})\n",
    "\n",
    "    \n",
    "    df_s['Result'] = df_s['Org_Sentence'].apply(process_sentence)\n",
    "    df_s[['Sentence', 'Tag']] = pd.DataFrame(df_s['Result'].tolist(), index=df_s.index)\n",
    "    df_s['Sentence'] = df_s['Sentence'].apply(lambda x: ' '.join(x))\n",
    "    df_s['Tag'] = df_s['Tag'].apply(lambda x: ' '.join(x))\n",
    "    # drop results column\n",
    "    df_s = df_s.drop(columns=['Result'])\n",
    "\n",
    "    df_s = drop_duplicates(df_s)\n",
    "    \n",
    "    # save to csv\n",
    "    df_s.to_csv(f'data/processed/phee/ace/{file_name}.csv', index=False)\n",
    "\n",
    "    return df_s\n",
    "\n",
    "for file_name in files:\n",
    "    preprocess(file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_t_capital(row):\n",
    "#     new_tags = []\n",
    "#     for tag, org_tag in zip(row['Tag'].split(), row['Org_Tokenized_Tag'].split()):\n",
    "#         if tag == 'I-test':\n",
    "#             new_tags.append('I-Test')\n",
    "#         else:\n",
    "#             new_tags.append(org_tag)\n",
    "#     return ' '.join(new_tags)\n",
    "\n",
    "# files = ['train', 'dev', 'test']\n",
    "# for file_name in files:\n",
    "#     df = pd.read_csv(f'data/processed/phee/ace/{file_name}.csv')\n",
    "#     # df_unequal contains the sentences that become different after bert tokenizer (e.g. org-> I' m , bert-> I ' m)\n",
    "#     df_unequal = df[(df['Tag'].str.split().apply(lambda x: len(x)) != df['Org_Tag'].str.split().apply(lambda x: len(x)))]\n",
    "#     df_unequal.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#     for i in range(len(df_unequal)):\n",
    "#         j = 0\n",
    "#         k = 0\n",
    "#         while j < len(df_unequal['Sentence'][i].split()):\n",
    "#             if df_unequal['Sentence'][i].split()[j].lower() != df_unequal['Org_Sentence'][i].split()[k].lower():\n",
    "            \n",
    "#                 # word in tokenized sentence\n",
    "#                 word = df_unequal['Sentence'][i].split()[j] \n",
    "#                 while(word.lower() != df_unequal['Org_Sentence'][i].split()[k].lower()):\n",
    "                    \n",
    "#                     # print('word: ', word, 'word2 ', df_unequal['Org_Sentence'][i].split()[k].lower())\n",
    "#                     word = word + df_unequal['Sentence'][i].split()[j+1]\n",
    "#                     str_arr = df_unequal['Org_Tag'][i].split()\n",
    "#                     str_arr.insert(k+1,  df_unequal['Org_Tag'][i].split()[k])\n",
    "#                     df_unequal.loc[i, 'Org_Tag'] = ' '.join(str_arr)\n",
    "#                     j = j + 1\n",
    "#             k = k + 1\n",
    "#             j = j + 1\n",
    "            \n",
    "#     # create a copy of df['Org_Tag'] column\n",
    "#     df['Org_Tokenized_Tag'] = df['Org_Tag'] \n",
    "\n",
    "#     update_values = df['Sentence_ID'].map(df_unequal.set_index('Sentence_ID')['Org_Tag'])\n",
    "#     df['Org_Tokenized_Tag'] = update_values.combine_first(df['Org_Tokenized_Tag'])  \n",
    "\n",
    "#     # I-test -> I-Test\n",
    "#     df['Org_Tokenized_Tag'] = df.apply(update_t_capital, axis=1)\n",
    "\n",
    "#     # save to csv\n",
    "#     df.to_csv(f'data/processed/phee/ace/{file_name}_w_test_tag.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # There is a new mapping check new_mapping.ipynb\n",
    "# treatment_tags = [\n",
    "#     \"I-Treatment.Drug\",\n",
    "#     \"I-Treatment.Combination.Drug\",\n",
    "#     \"I-Treatment\",\n",
    "#     \"I-Treatment.Route\",\n",
    "#     \"I-Treatment.Dosage\",\n",
    "#     \"I-Combination.Drug\",\n",
    "#     \"I-Treatment.Duration\",\n",
    "#     \"I-Dosage\",\n",
    "#     \"I-Freq\",\n",
    "#     \"I-Drug\",\n",
    "#     \"I-Treatment.Time_elapsed\",\n",
    "#     \"I-Treatment.Freq\"\n",
    "# ]\n",
    "\n",
    "# # Map to the new tag\n",
    "# treatment_mapping = {tag: \"I-Treatment\" for tag in treatment_tags}\n",
    "\n",
    "# problem_tags = [\n",
    "#     \"I-Effect\",\n",
    "#     \"I-Treat_Disorder\",\n",
    "#     \"I-Treatment.Treat_Disorder\",\n",
    "#     \"I-Subject.Sub_Disorder\",\n",
    "#     \"I-Sub_Disorder\"\n",
    "# ]\n",
    "\n",
    "# # Map to the new tag\n",
    "# problem_mapping = {tag: \"I-Problem\" for tag in problem_tags}\n",
    "\n",
    "# background_tags = [\n",
    "#     \"I-Subject.Age\",\n",
    "#     \"I-Subject.Gender\",\n",
    "#     \"I-Subject.Race\",\n",
    "#     \"I-Subject\",\n",
    "#     \"I-Race\",\n",
    "#     \"I-Gender\"\n",
    "# ]\n",
    "\n",
    "# # Map to the new tag\n",
    "# background_mapping = {tag: \"I-Background\" for tag in background_tags}\n",
    "\n",
    "# other_tags = [\n",
    "#     \"I-Duration\",\n",
    "#     \"I-Time_elapsed\"\n",
    "# ]\n",
    "\n",
    "# # Map to the new tag\n",
    "# other_mapping = {tag: \"I-Other\" for tag in other_tags}\n",
    "\n",
    "# o_tags = [\n",
    "#     \"I-Subject.Population\",\n",
    "#     \"I-Potential_therapeutic_event.Trigger\",\n",
    "#     \"I-Adverse_event.Trigger\",\n",
    "#     \"I-Route\",\n",
    "#     \"I-Population\",\n",
    "#     \"O\"\n",
    "# ]\n",
    "\n",
    "# # Map to the new tag\n",
    "# o_mapping = {tag: \"O\" for tag in o_tags}\n",
    "\n",
    "# test_tags = [\n",
    "#     \"I-Test\"\n",
    "# ]\n",
    "\n",
    "# test_mapping = {tag: \"I-Test\" for tag in test_tags}\n",
    "\n",
    "# # Combine all mappings\n",
    "# all_mappings = {**treatment_mapping, **problem_mapping, **background_mapping, **other_mapping, **o_mapping, **test_mapping}\n",
    "\n",
    "# def map_tags(tag_string):\n",
    "#     return ' '.join(all_mappings.get(tag, tag) for tag in tag_string.split())\n",
    "\n",
    "# # List of words to relabel as \"O\"\n",
    "# words_to_relabel = [\"a\", \"an\", \"and\", \"the\", \"with\", 'for', 'nor', 'but', 'or', 'yet']\n",
    "\n",
    "# # Function to change the label of specific words to \"O\"\n",
    "# def relabel_words(row):\n",
    "#     words = row['sentence'].split()\n",
    "#     tags = row['tag'].split()\n",
    "#     new_tags = []\n",
    "\n",
    "#     for word, tag in zip(words, tags):\n",
    "#         if word.lower() in words_to_relabel or word in string.punctuation:\n",
    "#             new_tags.append('O')\n",
    "#         else:\n",
    "#             new_tags.append(tag)\n",
    "    \n",
    "#     return ' '.join(new_tags)\n",
    "\n",
    "# files = ['train', 'dev', 'test']\n",
    "# for file_name in files:\n",
    "#     df_w_t = pd.read_csv(f'data/processed/phee/ace/{file_name}_w_test_tag.csv')\n",
    "#     df_w_t[\"Med_Tag\"] = df_w_t[\"Org_Tokenized_Tag\"].apply(map_tags)\n",
    "#     df_w_t = df_w_t[['Sentence', 'Med_Tag']]\n",
    "#     df_w_t.rename(columns={\"Sentence\": \"sentence\", \"Med_Tag\": \"tag\"}, inplace=True)\n",
    "\n",
    "#     # relabel punctuation and conjunctions as \"O\"\n",
    "#     df_w_t['tag'] = df_w_t.apply(relabel_words, axis=1)\n",
    "\n",
    "#     # save to csv\n",
    "#     df_w_t.to_csv(f'data/processed/phee/ace/{file_name}_w_test_tag_mapped.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    a 53 - year - old man developed lower leg edem...\n",
       "tag         O I-Background O I-Background O I-Background I...\n",
       "Name: 2789, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/processed/phee/ace/train_w_test_tag_mapped.csv')\n",
    "train_df.iloc[2789]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT BEING USED SAME IN trainer_clinical_bert.ipynb\n",
    "# # 60 train, 20 dev, 20 test\n",
    "# train_df = pd.read_csv('data/processed/phee/ace/train_w_test_tag_mapped.csv')\n",
    "# dev_df = pd.read_csv('data/processed/phee/ace/dev_w_test_tag_mapped.csv')\n",
    "# test_df = pd.read_csv('data/processed/phee/ace/test_w_test_tag_mapped.csv')\n",
    "\n",
    "\n",
    "# train_df = train_df[['Sentence', 'Med_Tag']]\n",
    "# train_df.rename(columns={\"Sentence\": \"sentence\", \"Med_Tag\": \"tag\"}, inplace=True)\n",
    "# dev_df = dev_df[['Sentence', 'Med_Tag']]\n",
    "# dev_df.rename(columns={\"Sentence\": \"sentence\", \"Med_Tag\": \"tag\"}, inplace=True)\n",
    "# test_df = test_df[['Sentence', 'Med_Tag']]\n",
    "# test_df.rename(columns={\"Sentence\": \"sentence\", \"Med_Tag\": \"tag\"}, inplace=True)\n",
    "\n",
    "# train_df['sentence'] = train_df['sentence'].apply(lambda x: x.split())\n",
    "# train_df['tag'] = train_df['tag'].apply(lambda x: x.split())\n",
    "# dev_df['sentence'] = dev_df['sentence'].apply(lambda x: x.split())\n",
    "# dev_df['tag'] = dev_df['tag'].apply(lambda x: x.split())\n",
    "# test_df['sentence'] = test_df['sentence'].apply(lambda x: x.split())\n",
    "# test_df['tag'] = test_df['tag'].apply(lambda x: x.split())\n",
    "\n",
    "# # save to csv\n",
    "# train_df.to_csv('data/processed/phee/ace/final_train.csv', index=False)\n",
    "# dev_df.to_csv('data/processed/phee/ace/final_dev.csv', index=False)\n",
    "# test_df.to_csv('data/processed/phee/ace/final_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
