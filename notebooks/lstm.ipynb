{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ozan/anaconda3/envs/lara-medh/lib/python3.12/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/ozan/anaconda3/envs/lara-medh/lib/python3.12/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import BertModel\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BertModel, BertConfig\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer,  EarlyStoppingCallback\n",
    "# from torchcrf import CRF\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import ast\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import optuna\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label2id = {'O': 0, 'I-Treatment': 1, 'I-Test': 2, 'I-Problem': 3, 'I-Background': 4, 'I-Other': 5}\n",
    "# id2label = {0: 'O', 1: 'I-Treatment', 2: 'I-Test', 3: 'I-Problem', 4: 'I-Background', 5: 'I-Other'}\n",
    "\n",
    "label2id = {'O': 0, 'I': 1}\n",
    "id2label = {0: 'O', 1: 'I'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(example_batch):\n",
    "    example_batch['sentence'] = ast.literal_eval(example_batch['sentence'])\n",
    "    example_batch['tag'] = ast.literal_eval(example_batch['tag'])\n",
    "    example_batch['tag'] = [label2id[label] for label in example_batch['tag']]\n",
    "    return example_batch\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"sentence\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    \n",
    "    all_labels = examples[\"tag\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def tokenize_and_align_labels_wrapper(examples, tokenizer):\n",
    "    return tokenize_and_align_labels(examples, tokenizer)\n",
    "\n",
    "def untokenize_labels_predictions(word_ids, true_labels, predictions):\n",
    "    untokenized_true_labels = []\n",
    "    untokenized_predictions = []\n",
    "\n",
    "    for sublist_word_ids, sublist_true_labels, sublist_predictions in zip(word_ids, true_labels, predictions):\n",
    "        current_labels = []\n",
    "        current_predictions = []\n",
    "        last_word_id = None\n",
    "\n",
    "        for word_id, label, prediction in zip(sublist_word_ids[1:-1], sublist_true_labels, sublist_predictions):\n",
    "            # Skip if this word_id is the same as the last one (it's a subword)\n",
    "            if word_id == last_word_id:\n",
    "                continue\n",
    "\n",
    "            current_labels.append(label)\n",
    "            current_predictions.append(prediction)\n",
    "            last_word_id = word_id\n",
    "\n",
    "        untokenized_true_labels.append(current_labels)\n",
    "        untokenized_predictions.append(current_predictions)\n",
    "\n",
    "    return untokenized_true_labels, untokenized_predictions\n",
    "\n",
    "class TrainingMonitor:\n",
    "    def __init__(self, word_ids):\n",
    "        self.best_f1 = 0\n",
    "        self.best_confusion_matrix = None\n",
    "        self.eval_dataset_name = \"phee\"\n",
    "        self.word_ids = word_ids\n",
    "    \n",
    "    def set_eval_dataset_name(self, eval_dataset_name):\n",
    "        self.eval_dataset_name = eval_dataset_name\n",
    "\n",
    "    def set_word_ids(self, word_ids):\n",
    "        self.word_ids = word_ids\n",
    "\n",
    "    def compute_metrics_factory(self, fold_no, model_name, binary_classification=False):\n",
    "        # Define the actual compute_metrics function\n",
    "        def compute_metrics(eval_preds):\n",
    "            logits, labels = eval_preds\n",
    "            predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "            # Remove ignored index (special tokens) and convert to labels\n",
    "            true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "            true_predictions = [\n",
    "                [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "\n",
    "            untokenized_true_labels, untokenized_predictions = untokenize_labels_predictions(self.word_ids, true_labels, true_predictions)\n",
    "\n",
    "            unflat_true = [label for seq in untokenized_true_labels for label in seq]\n",
    "            unflat_pred = [label for seq in untokenized_predictions for label in seq]\n",
    "            unreport = classification_report(y_pred=unflat_pred, y_true=unflat_true, output_dict=True)\n",
    "\n",
    "            if binary_classification:\n",
    "                scores = softmax(logits, axis=-1)\n",
    "\n",
    "                y_true = [[0 if l == 0 else 1 for l in label if l != -100] for label in labels]\n",
    "                y_scores_list = [[sum(s[1:]) for (s, l) in zip(score, label) if l != -100]\n",
    "                    for score, label in zip(scores, labels)] # list of lists\n",
    "                y_true = [item for sublist in y_true for item in sublist]\n",
    "                y_scores = [item for sublist in y_scores_list for item in sublist]\n",
    "\n",
    "                # Calculate ROC Curve and AUC\n",
    "                fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "\n",
    "                # Calculate Precision-Recall Curve and AUC\n",
    "                precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "                pr_auc = auc(recall, precision)\n",
    "\n",
    "                # Plot ROC Curve\n",
    "                plt.figure(figsize=(8, 8))\n",
    "                plt.plot(fpr, tpr, color='blue', lw=2, marker='.', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "                plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "                \n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title('ROC Curve')\n",
    "                plt.legend(loc=\"lower right\")\n",
    "                plt.savefig(f'analysis/{model_name}/{self.eval_dataset_name}/graphs/fold{fold_no}/binary_dataset_roc-curve.png')\n",
    "                plt.close()\n",
    "\n",
    "                # Plot Precision-Recall Curve\n",
    "                plt.figure(figsize=(8, 8))\n",
    "                plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve (area = %0.2f)' % pr_auc)\n",
    "            \n",
    "                plt.xlabel('Recall')\n",
    "                plt.ylabel('Precision')\n",
    "                plt.title('Precision-Recall Curve')\n",
    "                plt.legend(loc=\"lower left\")\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'analysis/{model_name}/{self.eval_dataset_name}/graphs/fold{fold_no}/binary_dataset_precision-recall-curve.png')\n",
    "                plt.close()\n",
    "\n",
    "\n",
    "                cm = confusion_matrix(y_pred=unflat_pred, y_true=unflat_true)\n",
    "                disp = ConfusionMatrixDisplay(cm, display_labels=np.array(['I', 'O']))\n",
    "                fig, ax = plt.subplots(figsize=(8, 8))\n",
    "                disp.plot(ax=ax)\n",
    "                \n",
    "                # Save the figure to an image file\n",
    "                plt.savefig(f'analysis/{model_name}/{self.eval_dataset_name}/graphs/fold{fold_no}/binary_confusion_matrix.png')\n",
    "                plt.close()\n",
    "\n",
    "                un_report_df = pd.DataFrame(unreport).round(3).T\n",
    "                with open(f\"analysis/{model_name}/{self.eval_dataset_name}/reports/fold{fold_no}/binary_dataset_classification_report.json\", \"w\") as f:\n",
    "                    json.dump(un_report_df.to_dict(), f, indent=4)\n",
    "\n",
    "                return {\n",
    "                    \"precision\": unreport['macro avg']['precision'],\n",
    "                    \"recall\": unreport['macro avg']['recall'],\n",
    "                    \"f1\": unreport['macro avg']['f1-score'],\n",
    "                    \"accuracy\": unreport['accuracy'],\n",
    "                }\n",
    "               \n",
    "            else:\n",
    "                unreport['macro_wo_O'] = {'precision': (unreport['I-Background']['precision'] + unreport['I-Other']['precision'] + unreport['I-Problem']['precision'] + unreport['I-Test']['precision'] + unreport['I-Treatment']['precision']) / 5,\n",
    "                'recall': (unreport['I-Background']['recall'] + unreport['I-Other']['recall'] + unreport['I-Problem']['recall'] + unreport['I-Test']['recall'] + unreport['I-Treatment']['recall']) / 5,\n",
    "                'f1-score': (unreport['I-Background']['f1-score'] + unreport['I-Other']['f1-score'] + unreport['I-Problem']['f1-score'] + unreport['I-Test']['f1-score'] + unreport['I-Treatment']['f1-score']) / 5,\n",
    "                'support': (unreport['I-Background']['support'] + unreport['I-Other']['support'] + unreport['I-Problem']['support'] + unreport['I-Test']['support'] + unreport['I-Treatment']['support'])}\n",
    "                \n",
    "                un_report_df = pd.DataFrame(unreport).round(3).T\n",
    "\n",
    "                binary_predictions = ['0' if label == 'O' else '1' for label in unflat_pred]\n",
    "                binary_labels = ['0' if label == 'O' else '1' for label in unflat_true]\n",
    "\n",
    "                # Generate a classification report\n",
    "                binary_classification_report = classification_report(y_true=binary_labels, y_pred=binary_predictions, target_names=['O', 'I'], digits=3, output_dict=True)\n",
    "\n",
    "           \n",
    "           \n",
    "                # new_f1_score = unreport['macro_wo_O']['f1-score']\n",
    "                # if self.best_f1 < new_f1_score:\n",
    "                #     self.best_f1 = new_f1_score\n",
    "                cm = confusion_matrix(y_pred=unflat_pred, y_true=unflat_true)\n",
    "                disp = ConfusionMatrixDisplay(cm, display_labels=np.array(['I-Background','I-Other', 'I-Problem', 'I-Test', 'I-Treatment', 'O']))\n",
    "                fig, ax = plt.subplots(figsize=(8, 8))\n",
    "                disp.plot(ax=ax)\n",
    "                \n",
    "                # Save the figure to an image file\n",
    "                plt.savefig(f'analysis/{model_name}/{self.eval_dataset_name}/graphs/fold{fold_no}/confusion_matrix.png')\n",
    "                plt.close()\n",
    "                \n",
    "                with open(f\"analysis/{model_name}/{self.eval_dataset_name}/reports/fold{fold_no}/multiclass_classification_report.json\", \"w\") as f:\n",
    "                    json.dump(un_report_df.to_dict(), f, indent=4)\n",
    "                \n",
    "                # multiclass is mapped to binary \n",
    "                with open(f\"analysis/{model_name}/{self.eval_dataset_name}/reports/fold{fold_no}/binary_classification_report.json\", \"w\") as f:\n",
    "                    json.dump(binary_classification_report, f, indent=4)\n",
    "                \n",
    "\n",
    "                return {\n",
    "                    \"precision\": unreport['macro_wo_O']['precision'],\n",
    "                    \"recall\": unreport['macro_wo_O']['recall'],\n",
    "                    \"f1\": unreport['macro_wo_O']['f1-score'],\n",
    "                    \"accuracy\": unreport['accuracy'],\n",
    "                }\n",
    "        return compute_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertLSTMModel(BertModel):  # (832x50 and 768x13)\n",
    "    def __init__(self, config, lstm_hidden_size, lstm_num_layers, lstm_dropout):\n",
    "        super().__init__(config)   \n",
    "        self.lstm = nn.LSTM(input_size=config.hidden_size, hidden_size=lstm_hidden_size, num_layers=lstm_num_layers, dropout=lstm_dropout if lstm_num_layers > 1 else 0, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Linear(lstm_hidden_size*2, config.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = super().forward(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        lstm_out, _ = self.lstm(sequence_output)\n",
    "        logits = self.classifier(lstm_out)\n",
    "\n",
    "        # Calculate the loss if labels are provided\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "    \n",
    "def model_init(trial, best_model_config=None):\n",
    "    # Define hyperparameters using the trial object\n",
    "    print(trial)\n",
    "    lstm_hidden_size = trial.suggest_categorical('lstm_hidden_size', [50, 100, 200]) if trial is not None else best_model_config['lstm_hidden_size']\n",
    "    lstm_num_layers = trial.suggest_categorical('lstm_num_layers', [1, 2, 3]) if trial is not None else best_model_config['lstm_num_layers']\n",
    "    lstm_dropout = trial.suggest_float('lstm_dropout', 0.1, 0.5) if trial is not None else best_model_config['lstm_dropout']\n",
    "    \n",
    "    config = BertConfig.from_pretrained('dmis-lab/biobert-v1.1', num_labels=2)\n",
    "    model = CustomBertLSTMModel(config, lstm_hidden_size=lstm_hidden_size, lstm_num_layers=lstm_num_layers, lstm_dropout=lstm_dropout)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial=None, best_model_config=None, binary_classification=True):\n",
    "    dataset_name = 'phee'\n",
    "    file_name_prefix = 'binary_' if binary_classification else \"\"\n",
    "    print(file_name_prefix)\n",
    "    for i in range(1):\n",
    "        if trial is not None:\n",
    "            # hyperparameter tuning mode\n",
    "            data = load_dataset('csv', data_files={'train': f'data/processed/{dataset_name}/fold{i}/{file_name_prefix}train.csv'})\n",
    "            dataset = data['train'].train_test_split(test_size=0.15, seed=42) # 85% training, 15% validation\n",
    "        else:\n",
    "            # best model training and evaluation mode\n",
    "            dataset = load_dataset('csv', data_files={'train': f'data/processed/{dataset_name}/fold{i}/{file_name_prefix}train.csv', 'test': f'data/processed/{dataset_name}/fold{i}/{file_name_prefix}test.csv'})\n",
    "\n",
    "        for file_type in ['train', 'test']:\n",
    "            dataset[file_type] = dataset[file_type].shuffle(seed=42).select(range(int(0.1 * len(dataset[file_type]))))  # Select 10%\n",
    "            dataset[file_type] = dataset[file_type].map(transform)\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-v1.1')\n",
    "        data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "        tokenized_datasets = dataset.map(\n",
    "        lambda examples: tokenize_and_align_labels_wrapper(examples, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=dataset['train'].column_names)\n",
    "\n",
    "        # word ids of the tokenized test set to find the original words\n",
    "        test_sentences = dataset['test']['sentence']\n",
    "        test_word_ids = []\n",
    "        for sentence in test_sentences:\n",
    "            test_word_ids.append(tokenizer(sentence, truncation=True, is_split_into_words=True).word_ids())\n",
    "\n",
    "\n",
    "        lstm_model = model_init(trial, best_model_config)\n",
    "\n",
    "\n",
    "        # Training arguments\n",
    "        lstm_training_args = TrainingArguments(\n",
    "            output_dir='./models/lstm',\n",
    "            num_train_epochs=2,\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            logging_strategy=\"epoch\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            overwrite_output_dir=True,\n",
    "            learning_rate=trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True) if trial is not None else best_model_config['learning_rate'],\n",
    "            metric_for_best_model=\"f1\",\n",
    "            load_best_model_at_end=True,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "\n",
    "        early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.01)\n",
    "        monitor = TrainingMonitor(test_word_ids)\n",
    "        # Trainer\n",
    "        lstm_trainer = Trainer(\n",
    "            model=lstm_model,\n",
    "            args=lstm_training_args,\n",
    "            train_dataset=tokenized_datasets['train'],\n",
    "            eval_dataset=tokenized_datasets['test'],\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=monitor.compute_metrics_factory(fold_no=i, model_name='lstm', binary_classification=binary_classification),\n",
    "            callbacks=[early_stopping_callback]\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        lstm_trainer.train()\n",
    "\n",
    "        if trial is not None:\n",
    "            eval_result = lstm_trainer.evaluate()\n",
    "            print(eval_result)\n",
    "            metric  = eval_result[\"eval_f1\"]\n",
    "            return metric\n",
    "        else:\n",
    "            for eval_dataset_name in ['mtsamples', 'doc-patient']:\n",
    "                print(f\"Evaluating on {eval_dataset_name}\")\n",
    "                eval_dataset = load_dataset('csv', data_files=f'data/processed/{eval_dataset_name}/{file_name_prefix}final_eval.csv')\n",
    "                eval_dataset = eval_dataset.map(transform)\n",
    "                \n",
    "                # word ids of the tokenized mtsamples to find the original words\n",
    "                eval_sentences = eval_dataset['train']['sentence']\n",
    "                eval_word_ids = []\n",
    "                for sentence in eval_sentences:\n",
    "                    eval_word_ids.append(tokenizer(sentence, truncation=True, is_split_into_words=True).word_ids())\n",
    "\n",
    "                eval_dataset = eval_dataset.map(lambda examples: tokenize_and_align_labels_wrapper(examples, tokenizer), batched=True, remove_columns=eval_dataset['train'].column_names)\n",
    "\n",
    "                lstm_trainer.eval_dataset = eval_dataset['train']\n",
    "                print(eval_dataset['train'])\n",
    "                monitor.set_eval_dataset_name(eval_dataset_name)\n",
    "                monitor.set_word_ids(eval_word_ids)\n",
    "                print(lstm_trainer.get_eval_dataloader(eval_dataset))\n",
    "                eval_result = lstm_trainer.evaluate() # compute metrics saves the confusion matrix and classification report\n",
    "                print(eval_result)\n",
    "                metric  = eval_result[\"eval_f1\"]\n",
    "                print(f\"Eval f1 on {eval_dataset_name}: {metric}\")\n",
    "            return metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-17 18:03:29,810] A new study created in memory with name: no-name-185b6c1b-7017-4ec7-9a70-642ed4dff2ff\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975b7b5843744fadb7c652db075a707c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<optuna.trial._trial.Trial object at 0x7f339b2cf2f0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 00:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.575700</td>\n",
       "      <td>0.426433</td>\n",
       "      <td>0.806420</td>\n",
       "      <td>0.811608</td>\n",
       "      <td>0.807977</td>\n",
       "      <td>0.810629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.394400</td>\n",
       "      <td>0.376540</td>\n",
       "      <td>0.816725</td>\n",
       "      <td>0.821401</td>\n",
       "      <td>0.818337</td>\n",
       "      <td>0.821108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-17 18:03:40,429] Trial 0 finished with value: 0.818336878744657 and parameters: {'lstm_hidden_size': 200, 'lstm_num_layers': 1, 'lstm_dropout': 0.19323491096507872, 'learning_rate': 3.843565531379673e-05}. Best is trial 0 with value: 0.818336878744657.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37653985619544983, 'eval_precision': 0.8167245141377713, 'eval_recall': 0.8214010371254188, 'eval_f1': 0.818336878744657, 'eval_accuracy': 0.8211077844311377, 'eval_runtime': 0.2551, 'eval_samples_per_second': 219.515, 'eval_steps_per_second': 54.879, 'epoch': 2.0}\n",
      "binary_\n",
      "<optuna.trial._trial.Trial object at 0x7f339b2cf2f0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 00:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.617600</td>\n",
       "      <td>0.494886</td>\n",
       "      <td>0.764306</td>\n",
       "      <td>0.770593</td>\n",
       "      <td>0.763424</td>\n",
       "      <td>0.764970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.442091</td>\n",
       "      <td>0.781249</td>\n",
       "      <td>0.787302</td>\n",
       "      <td>0.776332</td>\n",
       "      <td>0.776946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-17 18:03:50,164] Trial 1 finished with value: 0.7763320816451273 and parameters: {'lstm_hidden_size': 100, 'lstm_num_layers': 1, 'lstm_dropout': 0.2274142137625188, 'learning_rate': 1.4618507760222444e-05}. Best is trial 0 with value: 0.818336878744657.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4420906603336334, 'eval_precision': 0.7812488769092543, 'eval_recall': 0.7873020971960901, 'eval_f1': 0.7763320816451273, 'eval_accuracy': 0.7769461077844312, 'eval_runtime': 0.2401, 'eval_samples_per_second': 233.283, 'eval_steps_per_second': 58.321, 'epoch': 2.0}\n",
      "Number of finished trials:  2\n",
      "Best trial:\n",
      "  Value:  0.818336878744657\n",
      "  Params: \n",
      "{'lstm_hidden_size': 200, 'lstm_num_layers': 1, 'lstm_dropout': 0.19323491096507872, 'learning_rate': 3.843565531379673e-05}\n"
     ]
    }
   ],
   "source": [
    "# Initialize Optuna study and start the optimization process\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(lambda trial: objective(trial, binary_classification=True), n_trials=2)  # Adjust n_trials as needed\n",
    "\n",
    "# Best trial results\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "best_params = trial.params\n",
    "print(best_params)\n",
    "with open(\"models/lstm/best_binary_hyperparameters.json\", \"w\") as outfile:\n",
    "    json.dump(best_params, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/torch/nn/modules/rnn.py:769: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484657607/work/aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [94/94 00:27, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.615200</td>\n",
       "      <td>0.498000</td>\n",
       "      <td>0.751641</td>\n",
       "      <td>0.747526</td>\n",
       "      <td>0.710316</td>\n",
       "      <td>0.710400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.466800</td>\n",
       "      <td>0.448761</td>\n",
       "      <td>0.763460</td>\n",
       "      <td>0.773989</td>\n",
       "      <td>0.752017</td>\n",
       "      <td>0.753067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/torch/nn/modules/rnn.py:769: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484657607/work/aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on mtsamples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49be09b006cc48529b75dec475960858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9315a8839e489cb591dbff7c4d877b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f10cf5e2ef2450fb72bb7671cacba47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f32052205814a6b8656b1a202b1eb1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d193aed873584c32864c34cdfcc8895b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 101\n",
      "})\n",
      "<accelerate.data_loader.DataLoaderShard object at 0x7fb60acb2700>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/torch/nn/modules/rnn.py:769: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484657607/work/aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4467838704586029, 'eval_precision': 0.7506518737992509, 'eval_recall': 0.7646669711501866, 'eval_f1': 0.7454590174238691, 'eval_accuracy': 0.74830220713073, 'eval_runtime': 1.6535, 'eval_samples_per_second': 61.084, 'eval_steps_per_second': 7.862, 'epoch': 2.0}\n",
      "Eval f1 on mtsamples: 0.7454590174238691\n",
      "Evaluating on doc-patient\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989db99790174335a4cdf4a186b566f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190c8bcee65c4d8db6e30e19aacc71f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6853ce173484e6c8585017d7f6c688a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e074c14377ce480ea324c14a1bef0693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/88 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a36b9f69ad441e195d5e07260043922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/88 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 88\n",
      "})\n",
      "<accelerate.data_loader.DataLoaderShard object at 0x7fb5ec9c4460>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/torch/nn/modules/rnn.py:769: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484657607/work/aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7126114964485168, 'eval_precision': 0.5961882497166846, 'eval_recall': 0.6520208619640886, 'eval_f1': 0.5286400627512502, 'eval_accuracy': 0.5570578691184424, 'eval_runtime': 1.3663, 'eval_samples_per_second': 64.409, 'eval_steps_per_second': 8.051, 'epoch': 2.0}\n",
      "Eval f1 on doc-patient: 0.5286400627512502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5286400627512502"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the who train data for training with best hyperparameters\n",
    "# evaluate on test set\n",
    "\n",
    "best_model_config = json.load(open(\"models/lstm/best_binary_hyperparameters.json\", \"r\"))\n",
    "objective(None, best_model_config)  # Train and evaluate the best model\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
