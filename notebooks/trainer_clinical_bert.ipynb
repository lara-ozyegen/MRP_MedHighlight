{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import cuda\n",
    "import seqeval\n",
    "from seqeval.metrics import classification_report, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AdamW, Trainer, TrainingArguments\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60 train, 20 dev, 20 test\n",
    "train_df = pd.read_csv('data/processed/phee/ace/train_w_test_tag_mapped.csv')\n",
    "dev_df = pd.read_csv('data/processed/phee/ace/dev_w_test_tag_mapped.csv')\n",
    "test_df = pd.read_csv('data/processed/phee/ace/test_w_test_tag_mapped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[['Sentence', 'Med_Tag']]\n",
    "train_df.rename(columns={\"Sentence\": \"sentence\", \"Med_Tag\": \"tag\"}, inplace=True)\n",
    "dev_df = dev_df[['Sentence', 'Med_Tag']]\n",
    "dev_df.rename(columns={\"Sentence\": \"sentence\", \"Med_Tag\": \"tag\"}, inplace=True)\n",
    "test_df = test_df[['Sentence', 'Med_Tag']]\n",
    "test_df.rename(columns={\"Sentence\": \"sentence\", \"Med_Tag\": \"tag\"}, inplace=True)\n",
    "\n",
    "train_df['sentence'] = train_df['sentence'].apply(lambda x: x.split())\n",
    "train_df['tag'] = train_df['tag'].apply(lambda x: x.split())\n",
    "dev_df['sentence'] = dev_df['sentence'].apply(lambda x: x.split())\n",
    "dev_df['tag'] = dev_df['tag'].apply(lambda x: x.split())\n",
    "test_df['sentence'] = test_df['sentence'].apply(lambda x: x.split())\n",
    "test_df['tag'] = test_df['tag'].apply(lambda x: x.split())\n",
    "\n",
    "# save to csv\n",
    "train_df.to_csv('data/processed/phee/ace/final_train.csv', index=False)\n",
    "dev_df.to_csv('data/processed/phee/ace/final_dev.csv', index=False)\n",
    "test_df.to_csv('data/processed/phee/ace/final_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae01eeedb714c1f98570c5a59c81c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23b50deb29e4bb0866fa6202f6a93c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25c5593aa4646da9140f89594b47cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fdc4d27016f41b58516c580fc70f6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461d7cf9f19f46d7bf56dc6de16cb8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files={'train': 'data/processed/phee/ace/final_train.csv', 'validation': 'data/processed/phee/ace/final_dev.csv', 'test': 'data/processed/phee/ace/final_test.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label2id: {'O': 0, 'I-Treatment': 1, 'I-Test': 2, 'I-Problem': 3, 'I-Background': 4, 'I-Other': 5}\n",
      "id2label: {0: 'O', 1: 'I-Treatment', 2: 'I-Test', 3: 'I-Problem', 4: 'I-Background', 5: 'I-Other'}\n"
     ]
    }
   ],
   "source": [
    "unique_tags = pd.unique([tag for sublist in train_df['tag'] for tag in sublist])\n",
    "# Create label2id dictionary\n",
    "label2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "\n",
    "# Create id2label dictionary by swapping keys with values\n",
    "id2label = {id: tag for tag, id in label2id.items()}\n",
    "\n",
    "# Print the dictionaries\n",
    "print(\"label2id:\", label2id)\n",
    "print(\"id2label:\", id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88915917aee5491cbf003f3a39b59102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2793 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011a30d2fa1e49a28c5e8d04055b0e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/918 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883b75542656493f82228893e27a5657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/968 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform(example_batch):\n",
    "    example_batch['sentence'] = ast.literal_eval(example_batch['sentence'])\n",
    "    example_batch['tag'] = ast.literal_eval(example_batch['tag'])\n",
    "    example_batch['tag'] = [label2id[label] for label in example_batch['tag']]\n",
    "    return example_batch\n",
    "\n",
    "for type in ['train', 'validation', 'test']:\n",
    "    dataset[type] = dataset[type].map(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"samrawal/bert-base-uncased_clinical-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(dataset['train'][0]['sentence'], is_split_into_words=True)\n",
    "inputs.tokens()\n",
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 2, 2, 2, 2, 0, 0, 3, 3, 3, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels =  dataset['train'][0]['tag']\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"sentence\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"tag\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5e96d05a25484cb3b00f3ee6278dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2793 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e745edb08d4039a083b10307cb0158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/918 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4a703eab89494088ca76237c4b9036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/968 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       " 'validation': ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       " 'test': ['input_ids', 'token_type_ids', 'attention_mask', 'labels']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    1,    0,    0,    2,    2,    2,    2,    2,    0,    0,\n",
       "            3,    3,    3,    0,    0,    0,    0,    0,    0, -100],\n",
       "        [-100,    0,    2,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    4,    4,    4,    4,    0,    0,    0,    1,    1,    1,\n",
       "            1,    3,    3,    3,    0,    0,    0,    0,    0,    3,    3,    3,\n",
       "            3,    3,    3,    0,    0,    0, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets['train'][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    report = classification_report(y_true=true_labels, y_pred=true_predictions)\n",
    "    # save all_metrics to file\n",
    "    with open(\"analysis/reports/all_metrics.txt\", \"w\") as f:\n",
    "        f.write(str(true_predictions))\n",
    "        f.write(str(true_labels))\n",
    "        f.write(str(all_metrics))\n",
    "        # save report to file\n",
    "        f.write(report)\n",
    "\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at samrawal/bert-base-uncased_clinical-ner and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([7, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([7]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    'samrawal/bert-base-uncased_clinical-ner',\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# args = TrainingArguments(\n",
    "#     \"bert-finetuned-ner\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     num_train_epochs=5,\n",
    "#     weight_decay=0.01,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=10,\n",
    "    logging_strategy=\"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='880' max='880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [880/880 04:49, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.468367</td>\n",
       "      <td>0.481237</td>\n",
       "      <td>0.545424</td>\n",
       "      <td>0.511324</td>\n",
       "      <td>0.841077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.400800</td>\n",
       "      <td>0.405371</td>\n",
       "      <td>0.529126</td>\n",
       "      <td>0.593399</td>\n",
       "      <td>0.559423</td>\n",
       "      <td>0.858426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.342400</td>\n",
       "      <td>0.401999</td>\n",
       "      <td>0.541692</td>\n",
       "      <td>0.614495</td>\n",
       "      <td>0.575801</td>\n",
       "      <td>0.859112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.302600</td>\n",
       "      <td>0.376611</td>\n",
       "      <td>0.562386</td>\n",
       "      <td>0.628785</td>\n",
       "      <td>0.593735</td>\n",
       "      <td>0.868918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.383503</td>\n",
       "      <td>0.565112</td>\n",
       "      <td>0.634910</td>\n",
       "      <td>0.597981</td>\n",
       "      <td>0.869467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.245800</td>\n",
       "      <td>0.386023</td>\n",
       "      <td>0.562725</td>\n",
       "      <td>0.637972</td>\n",
       "      <td>0.597991</td>\n",
       "      <td>0.869673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.225100</td>\n",
       "      <td>0.395532</td>\n",
       "      <td>0.556808</td>\n",
       "      <td>0.638653</td>\n",
       "      <td>0.594929</td>\n",
       "      <td>0.869124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.213600</td>\n",
       "      <td>0.397743</td>\n",
       "      <td>0.559715</td>\n",
       "      <td>0.641034</td>\n",
       "      <td>0.597621</td>\n",
       "      <td>0.869364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.202000</td>\n",
       "      <td>0.401970</td>\n",
       "      <td>0.562743</td>\n",
       "      <td>0.639333</td>\n",
       "      <td>0.598598</td>\n",
       "      <td>0.870564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.197100</td>\n",
       "      <td>0.402319</td>\n",
       "      <td>0.563025</td>\n",
       "      <td>0.638312</td>\n",
       "      <td>0.598310</td>\n",
       "      <td>0.870393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ozan/.conda/envs/medh/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=880, training_loss=0.31166449893604625, metrics={'train_runtime': 290.8246, 'train_samples_per_second': 96.037, 'train_steps_per_second': 3.026, 'total_flos': 1004593751221392.0, 'train_loss': 0.31166449893604625, 'epoch': 10.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at samrawal/bert-base-uncased_clinical-ner and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([7, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([7]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    'samrawal/bert-base-uncased_clinical-ner',\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "Invalid user token. If you didn't pass a user token, make sure you are properly logged in by executing `huggingface-cli login`, and if you did pass a user token, double-check it's correct.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/medh/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:261\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    262\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.conda/envs/medh/lib/python3.8/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/whoami-v2",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/medh/lib/python3.8/site-packages/huggingface_hub/hf_api.py:954\u001b[0m, in \u001b[0;36mHfApi.whoami\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     hf_raise_for_status(r)\n\u001b[1;32m    955\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.conda/envs/medh/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:293\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    285\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    286\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m make sure you are authenticated.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m     )\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-65381619-3ded80db1c68c27f1590d208;9ea96ec6-2054-4602-910d-ecada659cd49)\n\nRepository Not Found for url: https://huggingface.co/api/whoami-v2.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ozan/lara/Med-Highlight/notebooks/trainer_clinical_bert.ipynb Cell 30\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.8.0.66/home/ozan/lara/Med-Highlight/notebooks/trainer_clinical_bert.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m \u001b[39mimport\u001b[39;00m Repository, get_full_repo_name\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.8.0.66/home/ozan/lara/Med-Highlight/notebooks/trainer_clinical_bert.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbert-finetuned-ner-accelerate\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.8.0.66/home/ozan/lara/Med-Highlight/notebooks/trainer_clinical_bert.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m repo_name \u001b[39m=\u001b[39m get_full_repo_name(model_name)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.8.0.66/home/ozan/lara/Med-Highlight/notebooks/trainer_clinical_bert.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m repo_name\n",
      "File \u001b[0;32m~/.conda/envs/medh/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/medh/lib/python3.8/site-packages/huggingface_hub/hf_api.py:4481\u001b[0m, in \u001b[0;36mHfApi.get_full_repo_name\u001b[0;34m(self, model_id, organization, token)\u001b[0m\n\u001b[1;32m   4479\u001b[0m         username \u001b[39m=\u001b[39m model_id\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m   4480\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 4481\u001b[0m         username \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwhoami(token\u001b[39m=\u001b[39;49mtoken)[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m]  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   4482\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00musername\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mmodel_id\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4483\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/medh/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/medh/lib/python3.8/site-packages/huggingface_hub/hf_api.py:956\u001b[0m, in \u001b[0;36mHfApi.whoami\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    954\u001b[0m     hf_raise_for_status(r)\n\u001b[1;32m    955\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 956\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(\n\u001b[1;32m    957\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInvalid user token. If you didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pass a user token, make sure you \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    958\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mare properly logged in by executing `huggingface-cli login`, and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    959\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mif you did pass a user token, double-check it\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms correct.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    960\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    961\u001b[0m \u001b[39mreturn\u001b[39;00m r\u001b[39m.\u001b[39mjson()\n",
      "\u001b[0;31mHTTPError\u001b[0m: Invalid user token. If you didn't pass a user token, make sure you are properly logged in by executing `huggingface-cli login`, and if you did pass a user token, double-check it's correct."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "model_name = \"bert-finetuned-ner-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Looks like you do not have git-lfs installed, please install. You can install from https://git-lfs.github.com/. Then run `git lfs install` (you only have to do this once).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/medh/lib/python3.8/site-packages/huggingface_hub/repository.py:573\u001b[0m, in \u001b[0;36mRepository.check_git_versions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 573\u001b[0m     lfs_version \u001b[39m=\u001b[39m run_subprocess(\u001b[39m\"\u001b[39;49m\u001b[39mgit-lfs --version\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlocal_dir)\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m    574\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/medh/lib/python3.8/site-packages/huggingface_hub/utils/_subprocess.py:83\u001b[0m, in \u001b[0;36mrun_subprocess\u001b[0;34m(command, folder, check, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m     folder \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(folder)\n\u001b[0;32m---> 83\u001b[0m \u001b[39mreturn\u001b[39;00m subprocess\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m     84\u001b[0m     command,\n\u001b[1;32m     85\u001b[0m     stderr\u001b[39m=\u001b[39;49msubprocess\u001b[39m.\u001b[39;49mPIPE,\n\u001b[1;32m     86\u001b[0m     stdout\u001b[39m=\u001b[39;49msubprocess\u001b[39m.\u001b[39;49mPIPE,\n\u001b[1;32m     87\u001b[0m     check\u001b[39m=\u001b[39;49mcheck,\n\u001b[1;32m     88\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     89\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mreplace\u001b[39;49m\u001b[39m\"\u001b[39;49m,  \u001b[39m# if not utf-8, replace char by �\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m     cwd\u001b[39m=\u001b[39;49mfolder \u001b[39mor\u001b[39;49;00m os\u001b[39m.\u001b[39;49mgetcwd(),\n\u001b[1;32m     91\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     92\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/medh/lib/python3.8/subprocess.py:493\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mstderr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m PIPE\n\u001b[0;32m--> 493\u001b[0m \u001b[39mwith\u001b[39;00m Popen(\u001b[39m*\u001b[39;49mpopenargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mas\u001b[39;00m process:\n\u001b[1;32m    494\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/medh/lib/python3.8/subprocess.py:858\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[1;32m    856\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m--> 858\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m    859\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m    860\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m    861\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m    862\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m    863\u001b[0m                         errread, errwrite,\n\u001b[1;32m    864\u001b[0m                         restore_signals, start_new_session)\n\u001b[1;32m    865\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/medh/lib/python3.8/subprocess.py:1720\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1719\u001b[0m         err_msg \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1720\u001b[0m     \u001b[39mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1721\u001b[0m \u001b[39mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'git-lfs'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/ozan/lara/Med-Highlight/notebooks/trainer_clinical_bert.ipynb Cell 30\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.8.0.66/home/ozan/lara/Med-Highlight/notebooks/trainer_clinical_bert.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m output_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbert-finetuned-ner-accelerate\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.8.0.66/home/ozan/lara/Med-Highlight/notebooks/trainer_clinical_bert.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m repo \u001b[39m=\u001b[39m Repository(output_dir, clone_from\u001b[39m=\u001b[39;49mrepo_name)\n",
      "File \u001b[0;32m~/.conda/envs/medh/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/medh/lib/python3.8/site-packages/huggingface_hub/repository.py:503\u001b[0m, in \u001b[0;36mRepository.__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, token, git_user, git_email, revision, skip_lfs_files, client)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip_lfs_files \u001b[39m=\u001b[39m skip_lfs_files\n\u001b[1;32m    501\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient \u001b[39m=\u001b[39m client \u001b[39mif\u001b[39;00m client \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m HfApi()\n\u001b[0;32m--> 503\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_git_versions()\n\u001b[1;32m    505\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(token, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    506\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhuggingface_token: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m token\n",
      "File \u001b[0;32m~/.conda/envs/medh/lib/python3.8/site-packages/huggingface_hub/repository.py:575\u001b[0m, in \u001b[0;36mRepository.check_git_versions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    573\u001b[0m     lfs_version \u001b[39m=\u001b[39m run_subprocess(\u001b[39m\"\u001b[39m\u001b[39mgit-lfs --version\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_dir)\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m    574\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m--> 575\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    576\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLooks like you do not have git-lfs installed, please install.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m You can install from https://git-lfs.github.com/.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Then run `git lfs install` (you only have to do this once).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m     )\n\u001b[1;32m    580\u001b[0m logger\u001b[39m.\u001b[39minfo(git_version \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m lfs_version)\n",
      "\u001b[0;31mOSError\u001b[0m: Looks like you do not have git-lfs installed, please install. You can install from https://git-lfs.github.com/. Then run `git lfs install` (you only have to do this once)."
     ]
    }
   ],
   "source": [
    "output_dir = \"bert-finetuned-ner-accelerate\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
